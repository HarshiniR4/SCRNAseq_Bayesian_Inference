{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de40370a-e172-4b1c-80e4-b7fd58b09e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import anndata\n",
    "import glob\n",
    "import gc\n",
    "import os\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b221d5e5-c641-4330-9700-4cc26bc06c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 h5ad files.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def is_valid_h5ad(filename):\n",
    "    \"\"\"Check if the file contains the expected 'obs' group.\"\"\"\n",
    "    try:\n",
    "        with h5py.File(filename, 'r') as f:\n",
    "            return \"obs\" in f.keys()\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening {filename}: {e}\")\n",
    "        return False\n",
    "\n",
    "# List all h5ad files in the current folder\n",
    "files = glob.glob(\"*.h5ad\")\n",
    "\n",
    "print(f\"Found {len(files)} h5ad files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68c94ab4-43d0-42f3-8286-29c3762db36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files= files[1:]\n",
    "files = ['GSE123025_GSM_preprocessed_with_metadata.h5ad',\n",
    " 'GSE121654_GSM_preprocessed_with_metadata.h5ad',\n",
    " 'GSE98969_GSM_preprocessed_with_metadata.h5ad',\n",
    "         'GSE98971_GSM_preprocessed_with_metadata.h5ad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f031adc4-d743-4e2d-8094-3d0d256a0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_union_vars(files):\n",
    "    \"\"\"Compute the union of all variable names across files without loading full data.\"\"\"\n",
    "    union_vars = set()\n",
    "    for f in files:\n",
    "        try:\n",
    "            # Open in backed mode to only read metadata\n",
    "            adata = sc.read_h5ad(f, backed='r')\n",
    "            print(adata.var_names)\n",
    "            union_vars.update(adata.var_names)\n",
    "            \n",
    "            adata.file.close()  # Close the file handle\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {f} for union vars: {e}\")\n",
    "    return list(union_vars)\n",
    "\n",
    "def reindex_adata(adata, union_vars):\n",
    "    \"\"\"\n",
    "    Reindex the AnnData object so that its columns match the union of all variables.\n",
    "    For missing variables, add columns filled with zeros.\n",
    "    \"\"\"\n",
    "    current_vars = adata.var_names.tolist()\n",
    "    missing = set(union_vars) - set(current_vars)\n",
    "    if missing:\n",
    "        n_missing = len(missing)\n",
    "        # Create a sparse matrix of zeros for the missing variables.\n",
    "        zeros = sparse.csr_matrix((adata.n_obs, n_missing))\n",
    "        # Create an AnnData object for missing columns.\n",
    "        missing_adata = anndata.AnnData(X=zeros, obs=adata.obs.copy(), \n",
    "                                        var=pd.DataFrame(index=list(missing)))\n",
    "        # Concatenate along columns (axis=1) with fill_value=0.\n",
    "        adata = anndata.concat([adata, missing_adata], axis=1, join='outer', fill_value=0)\n",
    "    # Now sort columns so that they follow the union_vars order.\n",
    "    adata = adata[:, union_vars].copy()\n",
    "    return adata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cfd0441-f844-4268-8074-58b8ffc99360",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['0610005C13Rik', '0610007C21Rik', '0610007L01Rik', '0610007N19Rik',\n",
      "       '0610007P08Rik', '0610007P14Rik', '0610007P22Rik', '0610008F07Rik',\n",
      "       '0610009B14Rik', '0610009B22Rik',\n",
      "       ...\n",
      "       'Zxda', 'Zxdb', 'Zxdc', 'Zyg11a', 'Zyg11b', 'Zyx', 'Zzef1', 'Zzz3', 'a',\n",
      "       'l7Rn6'],\n",
      "      dtype='object', length=23429)\n",
      "Index(['X0610007P14Rik', 'X0610009B22Rik', 'X0610009L18Rik', 'X0610009O20Rik',\n",
      "       'X0610010F05Rik', 'X0610010K14Rik', 'X0610011F06Rik', 'X0610012G03Rik',\n",
      "       'X0610025J13Rik', 'X0610030E20Rik',\n",
      "       ...\n",
      "       'Gm15882', 'Gm26671', 'Gm27196', 'Gm3289', 'Gm42485', 'Gm9857',\n",
      "       'Olfr1143', 'Pkhd1l1', 'Slc22a29', 'Tex19.2'],\n",
      "      dtype='object', name='GENE', length=20758)\n",
      "Index(['0610005C13Rik', '0610007C21Rik', '0610007L01Rik', '0610007P08Rik',\n",
      "       '0610007P14Rik', '0610007P22Rik', '0610008F07Rik', '0610009B14Rik',\n",
      "       '0610009B22Rik', '0610009D07Rik',\n",
      "       ...\n",
      "       'snoU83B', 'snoU85', 'snoU89', 'snoU90', 'snoU97', 'snoZ159', 'snoZ178',\n",
      "       'snoZ39', 'snoZ40', 'snosnR60_Z15'],\n",
      "      dtype='object', length=34016)\n",
      "Index(['0610005C13Rik', '0610007C21Rik', '0610007L01Rik', '0610007P08Rik',\n",
      "       '0610007P14Rik', '0610007P22Rik', '0610008F07Rik', '0610009B14Rik',\n",
      "       '0610009B22Rik', '0610009D07Rik',\n",
      "       ...\n",
      "       'snoU83B', 'snoU85', 'snoU89', 'snoU90', 'snoU97', 'snoZ159', 'snoZ178',\n",
      "       'snoZ39', 'snoZ40', 'snosnR60_Z15'],\n",
      "      dtype='object', length=34016)\n",
      "Union of variables has 39241 elements.\n"
     ]
    }
   ],
   "source": [
    "# Compute the union of variables from all files.\n",
    "union_vars = get_union_vars(files)\n",
    "print(f\"Union of variables has {len(union_vars)} elements.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e41b18-fadc-444d-94f3-1ce9348389fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: GSM3442052_P5_female_percoll_3.dge.txt_GSM_preprocessed_with_metadata.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hr2547/.local/lib/python3.11/site-packages/anndata/_core/anndata.py:1756: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n",
      "/home/hr2547/.local/lib/python3.11/site-packages/anndata/_core/anndata.py:1756: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: GSM1940063_Agilent_252665515669_S001_GE2_1105_Oct12_1_2.txt_GSM_preprocessed_with_metadata.h5ad\n",
      "Error processing GSM1940063_Agilent_252665515669_S001_GE2_1105_Oct12_1_2.txt_GSM_preprocessed_with_metadata.h5ad: \"Unable to open object (object 'obs' doesn't exist)\"\n",
      "Processing file: GSM2629424_AB2340.txt.gz_GSM_preprocessed_with_metadata.h5ad\n",
      "Processing file: GSE121654_GSM_preprocessed_with_metadata.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hr2547/.local/lib/python3.11/site-packages/anndata/_core/anndata.py:1756: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: GSM1940080_GSM_preprocessed_with_metadata.h5ad\n",
      "Error processing GSM1940080_GSM_preprocessed_with_metadata.h5ad: \"Unable to open object (object 'obs' doesn't exist)\"\n",
      "Processing file: GSE98971_GSM_preprocessed_with_metadata.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hr2547/.local/lib/python3.11/site-packages/anndata/_core/anndata.py:1756: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: GSM3442011_GSM_preprocessed_with_metadata.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hr2547/.local/lib/python3.11/site-packages/anndata/_core/anndata.py:1756: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: GSM2629424_GSM_preprocessed_with_metadata.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hr2547/.local/lib/python3.11/site-packages/anndata/_core/anndata.py:1756: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "# Iteratively concatenate files into one combined AnnData.\n",
    "combined_adata = None\n",
    "for f in files:\n",
    "    try:\n",
    "        print(f\"Processing file: {f}\")\n",
    "        # Open in backed mode and then load into memory.\n",
    "        adata = sc.read_h5ad(f, backed='r').to_memory()\n",
    "        # Ensure observation names are unique.\n",
    "        adata.obs_names_make_unique()\n",
    "        # Reindex so that this AnnData has the full set of variables.\n",
    "        adata = reindex_adata(adata, union_vars)\n",
    "        \n",
    "        # If this is the first file, initialize the combined object.\n",
    "    #     if combined_adata is None:\n",
    "    #         combined_adata = adata\n",
    "    #     else:\n",
    "    #         # Concatenate the new AnnData with the existing combined AnnData.\n",
    "    #         combined_adata = anndata.concat([combined_adata, adata],\n",
    "    #                                         join='outer', fill_value=0)\n",
    "    #     # Clean up to free memory.\n",
    "    #     del adata\n",
    "    #     gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {f}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81b79f8f-c514-41d8-9204-e0ba97176f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if combined_adata is None:\n",
    "    raise ValueError(\"No valid h5ad files were processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "360bddc8-b98c-43e3-bdde-b87c551c33f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined_adata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal combined shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mcombined_adata\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Save the final combined AnnData to disk.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m combined_adata\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined_data.h5ad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combined_adata' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Final combined shape:\", combined_adata.shape)\n",
    "# Save the final combined AnnData to disk.\n",
    "combined_adata.write(\"combined_data.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b71cb51-82b9-49d6-aa62-935ff163dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_obs_columns(files):\n",
    "    \"\"\"Compute the intersection of obs columns across all files without loading full data.\"\"\"\n",
    "    common_obs = None\n",
    "    for f in files:\n",
    "        try:\n",
    "            # Open in backed mode to quickly access metadata\n",
    "            adata = sc.read_h5ad(f, backed='r')\n",
    "            # Get the obs columns from this file\n",
    "            obs_cols = set(adata.obs.columns)\n",
    "            if common_obs is None:\n",
    "                common_obs = obs_cols\n",
    "            else:\n",
    "                common_obs = common_obs.intersection(obs_cols)\n",
    "            adata.file.close()  # Close file handle\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {f} for obs columns: {e}\")\n",
    "    return list(common_obs) if common_obs is not None else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "176671cd-0be8-4758-8302-2f3640fad8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reindex_obs(adata, common_obs_cols):\n",
    "    \"\"\"\n",
    "    Reindex the AnnData object's .obs DataFrame so that it contains exactly the columns in common_obs_cols.\n",
    "    For any missing column, create that column filled with NaN values.\n",
    "    \"\"\"\n",
    "    # Add missing columns with NaN values.\n",
    "    for col in common_obs_cols:\n",
    "        if col not in adata.obs.columns:\n",
    "            adata.obs[col] = np.nan\n",
    "\n",
    "    # Subset and reorder the obs DataFrame.\n",
    "    adata.obs = adata.obs[common_obs_cols].copy()\n",
    "    \n",
    "    return adata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b27ab0b2-5a57-435e-80d7-a039f11321cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common obs columns: ['DATASTORE region', 'BioProject', 'Consent', 'Bytes', 'DATASTORE filetype', 'Center Name', 'AvgSpotLen', 'ReleaseDate', 'Bases', 'Experiment', 'BioSample', 'DATASTORE provider', 'Organism', 'GEO_Accession (exp)', 'LibrarySource', 'strain', 'Assay Type', 'create_date', 'source_name', 'Instrument', 'version', 'LibraryLayout', 'LibrarySelection', 'Platform', 'SRA Study']\n"
     ]
    }
   ],
   "source": [
    "# Compute the common obs columns across files.\n",
    "common_obs_cols = get_common_obs_columns(files)\n",
    "print(\"Common obs columns:\", common_obs_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6730372-eafb-41ba-a0ff-8f8376c48f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_cols = [\n",
    "    'Bytes',\n",
    "    'Bases',\n",
    "    'strain',\n",
    "    'LibrarySelection',\n",
    "    'LibrarySource',\n",
    "    'LibraryLayout',\n",
    "    'Instrument',\n",
    "    'source_name',\n",
    "    'Assay Type',\n",
    "    'AvgSpotLen'\n",
    "]\n",
    "def keep_important_obs_cols(adata, important_cols):\n",
    "    \"\"\"\n",
    "    Subset the AnnData object's .obs to only the important columns.\n",
    "    Missing columns will be ignored.\n",
    "    \"\"\"\n",
    "    # Find which columns in your list actually exist in the data\n",
    "    existing_cols = [col for col in important_cols if col in adata.obs.columns]\n",
    "\n",
    "    # For columns that don't exist, you may want to add them as NaN\n",
    "    missing_cols = [col for col in important_cols if col not in adata.obs.columns]\n",
    "    for col in missing_cols:\n",
    "        adata.obs[col] = np.nan\n",
    "\n",
    "    # Now subset and reorder .obs to the exact order in important_cols\n",
    "    adata.obs = adata.obs[important_cols].copy()\n",
    "    return adata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7720aca-68f6-4b25-aab2-f68574950cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process files in small chunks and write intermediate combined AnnData files to disk.\n",
    "chunk_size = 1  # adjust as needed for your memory capacity\n",
    "intermediate_dir = \"intermediate_chunks\"\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "intermediate_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9efbd0e3-b714-4726-9b84-7d4b42067336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1: ['intermediate_chunks/chunk_3.h5ad']\n",
      "Written intermediate chunk to intermediate_chunks/chunk_0.h5ad\n",
      "Processing chunk 2: ['intermediate_chunks/chunk_2.h5ad']\n",
      "Written intermediate chunk to intermediate_chunks/chunk_1.h5ad\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(files), chunk_size):\n",
    "    combined_chunk = None\n",
    "    chunk_files = files[i:i+chunk_size]\n",
    "    print(f\"Processing chunk {i // chunk_size + 1}: {chunk_files}\")\n",
    "    for f in chunk_files:\n",
    "        try:\n",
    "            # Open in backed mode and load into memory.\n",
    "            adata = sc.read_h5ad(f, backed='r').to_memory()\n",
    "            adata.obs_names_make_unique()\n",
    "            # Reindex the .obs to have only the common obs columns.\n",
    "            # adata = reindex_obs(adata, common_obs_cols)\n",
    "            adata = keep_important_obs_cols(adata, important_cols)\n",
    "            # If this is the first file in the chunk, initialize; else, concatenate.\n",
    "            if combined_chunk is None:\n",
    "                combined_chunk = adata\n",
    "            else:\n",
    "                combined_chunk = anndata.concat([combined_chunk, adata],\n",
    "                                                join='outer', fill_value=0)\n",
    "            del adata\n",
    "            gc.collect()\n",
    "            time.sleep(2)  # short delay to alleviate I/O pressure\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {f}: {e}\")\n",
    "    if combined_chunk is not None:\n",
    "        chunk_file = os.path.join(intermediate_dir, f\"chunk_{i // chunk_size}.h5ad\")\n",
    "        combined_chunk.write(chunk_file)\n",
    "        intermediate_files.append(chunk_file)\n",
    "        print(f\"Written intermediate chunk to {chunk_file}\")\n",
    "        del combined_chunk\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d020cdc9-55fd-4eac-b469-ed35030a960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 37248 × 34016\n",
      "    obs: 'Bytes', 'Bases', 'strain', 'LibrarySelection', 'LibrarySource', 'LibraryLayout', 'Instrument', 'source_name', 'Assay Type', 'AvgSpotLen'\n",
      "AnnData object with n_obs × n_vars = 37248 × 34016\n",
      "    obs: 'Bytes', 'Bases', 'strain', 'LibrarySelection', 'LibrarySource', 'LibraryLayout', 'Instrument', 'source_name', 'Assay Type', 'AvgSpotLen'\n",
      "AnnData object with n_obs × n_vars = 37248 × 34016\n",
      "    obs: 'Bytes', 'Bases', 'strain', 'LibrarySelection', 'LibrarySource', 'LibraryLayout', 'Instrument', 'source_name', 'Assay Type', 'AvgSpotLen'\n",
      "AnnData object with n_obs × n_vars = 37248 × 34016\n",
      "    obs: 'Bytes', 'Bases', 'strain', 'LibrarySelection', 'LibrarySource', 'LibraryLayout', 'Instrument', 'source_name', 'Assay Type', 'AvgSpotLen'\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(\"intermediate_chunks/chunk_*.h5ad\")\n",
    "for f in files:\n",
    "    a1 = sc.read(f)\n",
    "    print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34ea6421-525e-4d48-a278-62aea5a2e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import anndata\n",
    "import zarr\n",
    "import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33f3787c-51b8-4c04-941a-387c8a623237",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"intermediate_chunks/chunk_*.h5ad\")\n",
    "\n",
    "zarr_paths = []\n",
    "for fn in files:\n",
    "    # Read the dataset in backed mode to avoid loading the entire file into memory\n",
    "    ad = sc.read_h5ad(fn, backed=\"r\")\n",
    "    zpath = fn.replace(\".h5ad\", \".zarr\")\n",
    "    \n",
    "    # Write to Zarr format with specified chunk sizes (tweak chunk sizes as needed)\n",
    "    ad.write_zarr(zpath, chunks=(500, 500))\n",
    "    \n",
    "    # Force the AnnData object to load fully into memory, then release it\n",
    "    ad._init_as_actual()  # Call without any parameters\n",
    "    del ad\n",
    "    gc.collect()\n",
    "    \n",
    "    zarr_paths.append(zpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2de5e86-ad5e-49d2-9a76-7b7c80afcea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done!\n",
      "  • merged X at: merged_dataset.zarr\n",
      "  • obs table at: merged_dataset.obs.csv\n",
      "  • var list at: merged_dataset.var.csv\n"
     ]
    }
   ],
   "source": [
    "import zarr, pandas as pd, numpy as np, glob, os, gc\n",
    "from collections import OrderedDict\n",
    "# 1) Locate your per‑chunk Zarr stores\n",
    "zarr_paths = sorted(glob.glob(\"intermediate_chunks/chunk_*.zarr\"))\n",
    "\n",
    "# 2) Build union of var names\n",
    "all_vars = []\n",
    "for zp in zarr_paths:\n",
    "    grp = zarr.open_group(zp, mode=\"r\")\n",
    "    # List the single key under var\n",
    "    keys = list(grp[\"var\"].array_keys())\n",
    "    if len(keys) != 1:\n",
    "        raise ValueError(f\"Expected exactly one var index in {zp}, found {keys}\")\n",
    "    idx_key = keys[0]\n",
    "    raw = grp[\"var\"][idx_key][:]\n",
    "    # Decode bytes to strings if necessary\n",
    "    vars_chunk = [v.decode(\"utf-8\") if isinstance(v, bytes) else v for v in raw]\n",
    "    all_vars.extend(vars_chunk)\n",
    "\n",
    "# Build union preserving first‐seen order\n",
    "union_vars = list(OrderedDict.fromkeys(all_vars))\n",
    "var2idx = {v: i for i, v in enumerate(union_vars)}\n",
    "total_vars = len(union_vars)\n",
    "\n",
    "# Step 3: Gather metadata including each chunk's gene list\n",
    "metas = []\n",
    "for zp in zarr_paths:\n",
    "    grp = zarr.open_group(zp, mode=\"r\")\n",
    "    X = grp[\"X\"]\n",
    "    var_keys = list(grp[\"var\"].array_keys())\n",
    "    idx_key = var_keys[0]\n",
    "    raw = grp[\"var\"][idx_key][:]\n",
    "    vars_chunk = [v.decode(\"utf-8\") if isinstance(v, bytes) else v for v in raw]\n",
    "\n",
    "    metas.append({\n",
    "        \"path\": zp,\n",
    "        \"n_obs\": X.shape[0],\n",
    "        \"chunks\": X.chunks,       # (row_chunk, col_chunk)\n",
    "        \"dtype\": X.dtype,\n",
    "        \"compressor\": X.compressor,\n",
    "        \"vars\": vars_chunk,\n",
    "    })\n",
    "\n",
    "total_obs = sum(m[\"n_obs\"] for m in metas)\n",
    "\n",
    "# Step 4: Create merged Zarr store for X\n",
    "out_path = \"merged_dataset1.zarr\"\n",
    "if os.path.exists(out_path):\n",
    "    os.remove(out_path)\n",
    "out = zarr.open_group(out_path, mode=\"w\")\n",
    "out.create_dataset(\n",
    "    \"X\",\n",
    "    shape=(total_obs, total_vars),\n",
    "    chunks=(metas[0][\"chunks\"][0], min(total_vars, 1000)),\n",
    "    dtype=metas[0][\"dtype\"],\n",
    "    compressor=metas[0][\"compressor\"],\n",
    ")\n",
    "\n",
    "# Step 5: Stream‐copy each chunk’s X into the global Zarr, aligning columns\n",
    "obs_dfs = []\n",
    "offset = 0\n",
    "\n",
    "for m in metas:\n",
    "    grp = zarr.open_group(m[\"path\"], mode=\"r\")\n",
    "    X = grp[\"X\"]\n",
    "    cols_idx = [var2idx[v] for v in m[\"vars\"]]\n",
    "\n",
    "    # collect this chunk's obs\n",
    "    obs_cols = {c: grp[\"obs\"][c][:] for c in grp[\"obs\"].array_keys()}\n",
    "    obs_dfs.append(pd.DataFrame(obs_cols))\n",
    "\n",
    "    # copy X in row‐chunks, slicing columns\n",
    "    row_chunk = m[\"chunks\"][0]\n",
    "    for start in range(0, m[\"n_obs\"], row_chunk):\n",
    "        stop = min(start + row_chunk, m[\"n_obs\"])\n",
    "        block = X[start:stop, :]  # shape (r, original_vars)\n",
    "    \n",
    "        # Assign each local column j to the global column cols_idx[j]\n",
    "        for j, gcol in enumerate(cols_idx):\n",
    "            out[\"X\"][offset + start : offset + stop, gcol] = block[:, j]\n",
    "    offset += m[\"n_obs\"]\n",
    "    gc.collect()\n",
    "\n",
    "# Step 6: Write obs and var out as CSVs (outside the Zarr store)\n",
    "obs_df = pd.concat(obs_dfs, axis=0, ignore_index=True)\n",
    "obs_df.to_csv(\"merged_dataset.zarr/merged_dataset.obs.csv\", index=False)\n",
    "\n",
    "pd.DataFrame(index=union_vars).to_csv(\"merged_dataset.zarr/merged_dataset.var.csv\")\n",
    "\n",
    "print(\"✅ Done!\")\n",
    "print(\"  • merged X at: merged_dataset.zarr\")\n",
    "print(\"  • obs table at: merged_dataset.obs.csv\")\n",
    "print(\"  • var list at: merged_dataset.var.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af9cba1b-de33-461f-8550-b8e42dba1160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 74496 × 34016\n",
      "    obs: 'AvgSpotLen', 'Bases', 'Bytes', '_index'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hr2547/myenv/lib64/python3.9/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "import anndata\n",
    "import zarr\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Load obs and var\n",
    "obs = pd.read_csv(\"merged_dataset.zarr/merged_dataset.obs.csv\", index_col=None)\n",
    "var = pd.read_csv(\"merged_dataset.zarr/merged_dataset.var.csv\", index_col=0)\n",
    "\n",
    "# 2) Load X fully into memory (this will allocate a dense array!)\n",
    "X = zarr.open(\"merged_dataset1.zarr\", mode=\"r\")[\"X\"][:]\n",
    "\n",
    "# 3) Construct AnnData\n",
    "adata = anndata.AnnData(X=X, obs=obs, var=var)\n",
    "print(adata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8996c8d-ed28-40b8-9630-1a4199219865",
   "metadata": {},
   "outputs": [],
   "source": [
    "if '_index' in adata.obs.columns:\n",
    "    adata.obs.rename(columns={'_index': 'original_index'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec055caa-1868-45cc-8468-e70921f3396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) (Optional) write to h5ad for future use\n",
    "adata.write_h5ad(\"merged_dataset1.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1366d824-595f-49df-9cd4-89647e5a8b70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
